---
title: Confidence Intervals via Bootstrapping
output:
  html_document:
    theme: null
    highlight: null
    css: ../styles/dsairm.css
    fig_caption: true
    mathjax: default 
    keep_md: false
    includes:
      #in_header: in_header.txt
      before_body: ../styles/dsairm_before_body.txt
      after_body: ../styles/dsairm_after_body.txt 
bibliography: ../media/references.bib
---

##Overview {#shinytab1}
This app illustrates the method of boostrapping data, which is one way one can obtain confidence intervals for parameters when fitting mechanistic models to data.


##The Model {#shinytab2} 

The data and the underlying simulation model are the same as in the 'Basic model fitting' app. If you haven't worked your way through that app, do that first.

###Fitting Approach
This app allows you to fit the parameters either in linear or log space. Fitting in log space can be useful if you have a large range of parameters you want to try. The underlying problem is the same, no matter what scale you fit your parameters on, but sometimes one version or the other can lead to better performance of your solver.

Note that while choosing to fit parameters on a linear or log scale is just done to optimize performance of the computer code, a decision to fit your data on a linear or log scale corresponds to different biological problems and underlying assumptions about the processes that generated the data and any uncertainty/noise. For this app, data is fit on a log scale. 

###Bootstrapping
The main new feature of this app is the inclusion of bootstrapping, a sampling process that allows one to obtain confidence intervals for the estimated parameters. The basic approach goes like this:

* Resample your data (with replaclement), to get a new dataset as large as the original one. For instance if you had 20 data points in the original dataset, you'll again have 20 data points. But now some of them might occur more than once, and others might be missing. This approach tries to mimic the idea that if you had done another experiment, you might have gotten slightly different data.
* Fit your model to this 'new' dataset you obtained through sampling. Record the best-fit estimates for your parameters.
* Do this 'sample, then fit' approach multiple times (generally 100s or more) to obtain distribution for your parameter estimates.
* From these distributions, extract e.g. 95% confidence intervals.

Bootstrapping is conceptually easy to understand and also fairly easy to implement. The big drawback is that it takes time to run. Instead of fitting your model (until it converges) once, you now have to do it many times. That can take time and usually requires a mix of fast computers, parallelization, good/quick optimizers and simple models. For our teaching model, we'll use a simple model, only few bootstrap samples and a limited number of interations per fit. 


##What to do {#shinytab3}

_To come_



##Further Information {#shinytab4}
* The R script running the simulation for this app is called `simulate_fitconfint.R`. See its help file for information on how to call and use it directly.
* There are different versions of bootstrapping, i.e. data sampling. Some are very useful for time-series data, e.g. the concept of block-bootstrapping. See for instance .


### References


