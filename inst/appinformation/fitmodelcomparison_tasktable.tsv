"QuizID"	"AppID"	"AppTitle"	"TaskID"	"TaskText"	"RecordID"	"Record"	"Type"	"Note"
"DSAIRM_fitmodelcomparison"	"fitmodelcomparison"	"Model Comparison"	1	"Take a look at the inputs and outputs for the app. It's similar to the 'Basic Fitting' app (which you should do before this one). Each model has 3 parameters that are being estimated, the best fit estimates are shown under the figure, together with the SSR and AICc. Switch to model 2, run and compare how it differs from model 1."	"T1R1"	"Something"	"Rounded_Integer"	"Report the rounded integer"
"DSAIRM_fitmodelcomparison"	"fitmodelcomparison"	"Model Comparison"	2	"You probably noticed that the default setting is only 10 iterations for the solver. That's way too few to find the best fit, so comparing the 2 models after such few iterations is not very meaningful. Increase the iterations to 100 or more and re-run both models. You'll likely find that even with 100 iterations, the fits don't look that great. Explore different starting values for the parameters that are being fit (a,b and r/dX) and see if you can get better fits. You can also explore different values for the fixed parameters. Usually, those should not be changed since they are fixed based in biological a priori information. If we didn't know those parameter values, we would also try to fit them - though that can lead to overfitting, as you learned in another of the fitting apps. For this exercise, you can play with those fixed values."	"T2R1"	"Something"	"Rounded_Integer"	"Report the rounded integer"
"DSAIRM_fitmodelcomparison"	"fitmodelcomparison"	"Model Comparison"	3	"Based on the AICc for both models after 100 or more iterations, what do you conclude regarding the quality of each model? Take a look at SSR for both models. You should see the model with the lower AICc also have the lower SSR. Why is that so? Is that always true for any 2 models? (It's not: The 2 models here are in some sense special, understand how/why.) If you haven't figured out the last question, and also for a general check, compute AICc 'by hand' for each of the models using the reported SSR and the formula given in the 'Models' section. You should of course find the values shown below the plot. By doing all this, you should have realized that since the models have the same numbers of parameters (and are fit to the same data, which always has to be the case if you want to compare models), all the terms in the AICc equation other than SSR are the same for the 2 models. Thus, in this case, SSR and AICc behave the same. If one of the models had more parameters than the other, you could find a model with lower SSR but higher AIC (because the additional complexity of the model, evidenced by more parameters, doesn't lower the SSR enough for the AIC to go down.)"	"T3R1"	"Something"	"Rounded_Integer"	"Report the rounded integer"
